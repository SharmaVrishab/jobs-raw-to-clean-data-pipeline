# ğŸš€ Job Listings Data Pipeline

> A production-ready ETL pipeline that fetches job listings from a public API, validates data quality with dbt, and orchestrates daily runs using Apache Airflow.

![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-14+-336791.svg)
![Airflow](https://img.shields.io/badge/Airflow-2.x-017CEE.svg)
![dbt](https://img.shields.io/badge/dbt-1.5+-FF694B.svg)

---

## ğŸ“– Overview

This pipeline solves a common data engineering challenge: **how to reliably ingest, clean, and validate external API data for analytics**.

**What it does:**

- Fetches job listings daily from a public API
- Stores raw JSON for complete data lineage
- Transforms messy data into clean, structured tables
- Enforces data quality with automated dbt tests
- Fails fast when data doesn't meet standards

**Why it matters:** Downstream analysts can trust the data is complete, deduplicated, and always up-to-date.

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Jobs API      â”‚  External data source
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Python Ingestionâ”‚  Fetch & store raw JSON
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  raw_jobs       â”‚  PostgreSQL (JSONB storage)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SQL Transform   â”‚  Clean, normalize, dedupe
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ cleaned_jobs    â”‚  PostgreSQL (structured tables)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   dbt Tests     â”‚  Validate data quality
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Airflow DAG    â”‚  Orchestrate & monitor
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Tech Stack:**

- **Python** â†’ API ingestion
- **PostgreSQL** â†’ Data storage & SQL transformations
- **Airflow** â†’ Orchestration, scheduling, retries
- **dbt** â†’ Data quality enforcement

---

## ğŸ”„ Pipeline Flow

| Step | Action                              | Output              |
| ---- | ----------------------------------- | ------------------- |
| 1ï¸âƒ£   | Airflow triggers daily at 2 AM UTC  | Pipeline starts     |
| 2ï¸âƒ£   | Python script fetches jobs from API | Raw JSON stored     |
| 3ï¸âƒ£   | SQL extracts & normalizes fields    | Structured records  |
| 4ï¸âƒ£   | UPSERT logic deduplicates data      | Clean table updated |
| 5ï¸âƒ£   | dbt runs validation tests           | Pass/Fail status    |
| 6ï¸âƒ£   | Pipeline succeeds or fails          | Alert sent          |

**Key Feature:** Fully idempotentâ€”safe to re-run without duplicates.

---

## âœ… Data Quality (dbt)

### What Gets Validated

Every run checks the `cleaned_jobs` table for:

| Test               | Column         | Rule              |
| ------------------ | -------------- | ----------------- |
| ğŸ”‘ **Primary Key** | `job_id`       | NOT NULL + UNIQUE |
| ğŸ¢ **Company**     | `company_name` | NOT NULL          |
| ğŸ’¼ **Job Title**   | `title`        | NOT NULL          |

### dbt Schema Definition

```yaml
models:
  - name: cleaned_jobs
    columns:
      - name: job_id
        tests:
          - not_null
          - unique
      - name: company_name
        tests:
          - not_null
      - name: title
        tests:
          - not_null
```

### What Happens on Failure

âŒ If ANY test fails â†’ **Airflow DAG fails**
âœ… Only clean, validated data reaches production

This guarantees analysts never query incomplete or duplicate records.

---

## ğŸš€ Quick Start

### Prerequisites

```bash
Python 3.10+  |  PostgreSQL 14+  |  Airflow 2.x  |  dbt 1.5+
```

### Installation

```bash
# 1. Clone the repo
git clone https://github.com/yourusername/job-listings-pipeline.git
cd job-listings-pipeline

# 2. Install dependencies
pip install -r requirements.txt

# 3. Set up database
psql -U postgres -f sql/schema/001_create_raw_jobs.sql
psql -U postgres -f sql/schema/002_create_cleaned_jobs.sql

# 4. Configure Airflow connection (see docs)
```

### Run the Pipeline

```bash
# Trigger Airflow DAG
airflow dags trigger job_listings_pipeline

# Or run components individually
python scripts/fetch_jobs.py        # Ingestion only
dbt test --select cleaned_jobs      # Tests only
```

---

## ğŸ¯ Design Decisions

### What I Prioritized

| Decision                     | Rationale                               |
| ---------------------------- | --------------------------------------- |
| **SQL over Pandas**          | Better performance, easier to review    |
| **Raw data preservation**    | Enables reprocessing & auditing         |
| **Simple Airflow operators** | More maintainable than complex TaskFlow |
| **dbt for validation only**  | Keeps transformations in plain SQL      |
| **Idempotent UPSERT**        | Safe for backfills & re-runs            |

### What I Skipped (Intentionally)

- âŒ Kubernetes / Docker orchestration â†’ Not needed at this scale
- âŒ Real-time streaming â†’ Batch daily is sufficient
- âŒ Advanced partitioning â†’ Current volume doesn't require it
- âŒ Complex monitoring â†’ Airflow alerts cover basics

**Philosophy:** Start simple, scale when needed.

---

## ğŸ“Š Example Queries

### Top Hiring Companies

```sql
SELECT company_name, COUNT(*) as jobs
FROM cleaned_jobs
GROUP BY company_name
ORDER BY jobs DESC
LIMIT 10;
```

### Remote Work Trends

```sql
SELECT
    remote_type,
    COUNT(*) as total,
    ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 1) as pct
FROM cleaned_jobs
GROUP BY remote_type;
```

### Salary Insights

```sql
SELECT
    employment_type,
    AVG((salary_min + salary_max) / 2) as avg_salary
FROM cleaned_jobs
WHERE salary_min IS NOT NULL
GROUP BY employment_type
ORDER BY avg_salary DESC;
```

### Data Freshness Check

```sql
SELECT
    MAX(scraped_at) as last_update,
    COUNT(DISTINCT job_id) as unique_jobs,
    COUNT(*) as total_records
FROM cleaned_jobs;
```

---

## ğŸ“ Project Structure

```
job-listings-pipeline/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ job_listings_dag.py      # Airflow DAG definition
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ fetch_jobs.py            # API ingestion
â”‚   â””â”€â”€ clean_jobs.py            # Data transformation
â”œâ”€â”€ models/
â”‚   â””â”€â”€ schema.yml               # dbt tests
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ schema/                  # Table DDL
â”‚   â””â”€â”€ transforms/              # Cleaning logic
â””â”€â”€ README.md
```

---

## ğŸ¤ Contributing

Contributions welcome! Please:

1. Fork the repo
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ™‹ Questions?

Open an issue .

---

<div align="center">

â­ Star this repo if you find it useful!

</div>
