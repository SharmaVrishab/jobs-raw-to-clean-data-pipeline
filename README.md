# ğŸš€ Job Listings Data Pipeline

> A production-ready ETL pipeline that fetches job listings from a public API, validates data quality with dbt, and orchestrates daily runs using Apache Airflow.

![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-14+-336791.svg)
![Airflow](https://img.shields.io/badge/Airflow-2.x-017CEE.svg)
![dbt](https://img.shields.io/badge/dbt-1.5+-FF694B.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)

---

## ğŸ“– Overview

This pipeline solves a common data engineering challenge: **how to reliably ingest, clean, and validate external API data for analytics**.

**What it does:**
- Fetches job listings daily from a public API
- Stores raw JSON for complete data lineage
- Transforms messy data into clean, structured tables
- Enforces data quality with automated dbt tests
- Fails fast when data doesn't meet standards

**Why it matters:** Downstream analysts can trust the data is complete, deduplicated, and always up-to-date.

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Jobs API      â”‚  External data source
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Python Ingestionâ”‚  Fetch & store raw JSON
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  raw_jobs       â”‚  PostgreSQL (JSONB storage)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SQL Transform   â”‚  Clean, normalize, dedupe
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ cleaned_jobs    â”‚  PostgreSQL (structured tables)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   dbt Tests     â”‚  Validate data quality
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Airflow DAG    â”‚  Orchestrate & monitor
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Tech Stack:**
- **Python** â†’ API ingestion
- **PostgreSQL** â†’ Data storage & SQL transformations  
- **Airflow** â†’ Orchestration, scheduling, retries
- **dbt** â†’ Data quality enforcement

---

## ğŸ”„ Pipeline Flow

| Step | Action | Output |
|------|--------|--------|
| 1ï¸âƒ£ | Airflow triggers daily at 2 AM UTC | Pipeline starts |
| 2ï¸âƒ£ | Python script fetches jobs from API | Raw JSON stored |
| 3ï¸âƒ£ | SQL extracts & normalizes fields | Structured records |
| 4ï¸âƒ£ | UPSERT logic deduplicates data | Clean table updated |
| 5ï¸âƒ£ | dbt runs validation tests | Pass/Fail status |
| 6ï¸âƒ£ | Pipeline succeeds or fails | Alert sent |

**Key Feature:** Fully idempotentâ€”safe to re-run without duplicates.

---

## âœ… Data Quality (dbt)

### What Gets Validated

Every run checks the `cleaned_jobs` table for:

| Test | Column | Rule |
|------|--------|------|
| ğŸ”‘ **Primary Key** | `job_id` | NOT NULL + UNIQUE |
| ğŸ¢ **Company** | `company_name` | NOT NULL |
| ğŸ’¼ **Job Title** | `title` | NOT NULL |

### dbt Schema Definition

```yaml
models:
  - name: cleaned_jobs
    columns:
      - name: job_id
        tests:
          - not_null
          - unique
      - name: company_name
        tests:
          - not_null
      - name: title
        tests:
          - not_null
```

### What Happens on Failure

âŒ If ANY test fails â†’ **Airflow DAG fails**  
âœ… Only clean, validated data reaches production

This guarantees analysts never query incomplete or duplicate records.

---

## ğŸ¯ Design Decisions

### What I Prioritized

| Decision | Rationale |
|----------|-----------|
| **SQL over Pandas** | Better performance, easier to review |
| **Raw data preservation** | Enables reprocessing & auditing |
| **Simple Airflow operators** | More maintainable than complex TaskFlow |
| **dbt for validation only** | Keeps transformations in plain SQL |
| **Idempotent UPSERT** | Safe for backfills & re-runs |

### What I Skipped (Intentionally)

- âŒ Kubernetes / Docker orchestration â†’ Not needed at this scale
- âŒ Real-time streaming â†’ Batch daily is sufficient
- âŒ Advanced partitioning â†’ Current volume doesn't require it
- âŒ Complex monitoring â†’ Airflow alerts cover basics

**Philosophy:** Start simple, scale when needed.

---

## ğŸ“Š Example Query

```sql
-- Find top companies hiring right now
SELECT 
    company_name, 
    COUNT(*) as total_jobs
FROM cleaned_jobs
GROUP BY company_name
ORDER BY total_jobs DESC
LIMIT 10;
```

---

## ğŸ“ Project Structure

```
job-listings-pipeline/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ job_listings_dag.py      # Airflow DAG definition
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ fetch_jobs.py            # API ingestion
â”‚   â””â”€â”€ clean_jobs.py            # Data transformation
â”œâ”€â”€ models/
â”‚   â””â”€â”€ schema.yml               # dbt tests
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ schema/                  # Table DDL
â”‚   â””â”€â”€ transforms/              # Cleaning logic
â””â”€â”€ README.md
```

---

## ğŸ¤ Contributing

Contributions welcome! Please:

1. Fork the repo
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) for details.

---

## ğŸ™‹ Questions?

Open an issue or reach out at **your.email@example.com**

---

<div align="center">

**Built with â¤ï¸ by [Your Name]**

â­ Star this repo if you find it useful!

</div>
